{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tensorflow==2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/uber/causalml.git\n",
    "!cd causalml\n",
    "!pip install -r requirements.txt\n",
    "!pip install causalml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import integrate, sparse\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "%pylab inline\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "from joblib import Parallel, delayed\n",
    "from time import time\n",
    "import subprocess\n",
    "from uuid import uuid4\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit, KFold\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder, StandardScaler\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.datasets import dump_svmlight_file, load_svmlight_file\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression, SGDClassifier, SGDRegressor, Lasso, Ridge\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_val_score \n",
    "from sklearn.metrics import make_scorer, SCORERS, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from causalml.inference.meta import BaseXRegressor\n",
    "\n",
    "\n",
    "from math import factorial\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_criteo(N_sample):\n",
    "    \n",
    "    print(\"Loading Criteo dataset...\")\n",
    "    original_criteo_df = pd.read_csv('criteo-research-uplift-v2.1.csv.gz', \n",
    "                                     compression='gzip', \n",
    "                                     header=0, \n",
    "                                     sep=',', \n",
    "                                     quotechar='\"', \n",
    "                                     error_bad_lines=False)\n",
    "    print(\"Criteo dataset DONE !\")\n",
    "    \n",
    "    criteo_df = original_criteo_df.sample(n=N_sample) #int(len(original_criteo_df))\n",
    "    \n",
    "    del original_criteo_df\n",
    "    \n",
    "    t = criteo_df['treatment'].to_numpy()\n",
    "    y = criteo_df['visit'].to_numpy()\n",
    "    \n",
    "    list_categorical_feature = [1,3,4,5,6,8,9,11]\n",
    "    name_categorical_feature = [\"f\"+str(i) for i in list_categorical_feature]\n",
    "\n",
    "    list_categorical_feature_selected = [1,3,4,5,6,8,9,11]\n",
    "    name_categorical_feature_selected = [\"f\"+str(i) for i in list_categorical_feature_selected]\n",
    "\n",
    "    name_numerical_feature = [\"f\"+str(i) for i in range(12) if not(i in list_categorical_feature)]\n",
    "\n",
    "    X = criteo_df.drop(columns = ['treatment', 'conversion', 'visit', 'exposure']).to_numpy()\n",
    "    \n",
    "    return X, t, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hashed_criteo(nb_categorical_features_kept):\n",
    "    \n",
    "    print(\"Loading Criteo dataset...\")\n",
    "    original_criteo_df = pd.read_csv('criteo-research-uplift-v2.1.csv.gz', \n",
    "                                     compression='gzip', \n",
    "                                     header=0, \n",
    "                                     sep=',', \n",
    "                                     quotechar='\"', \n",
    "                                     error_bad_lines=False)\n",
    "    \n",
    "    X_total = sparse.load_npz('CU2_cat_hashed_100_feats.npz').toarray()\n",
    "    \n",
    "    print(\"Criteo dataset DONE !\")\n",
    "    \n",
    "    std_scaler = StandardScaler()\n",
    "    X_numerical = std_scaler.fit_transform(X_total[:,:4])\n",
    "\n",
    "    one_hot = OneHotEncoder()\n",
    "    X_categorical = one_hot.fit_transform(X_total[:, 4:(4+nb_categorical_features_kept)]).toarray()\n",
    "    \n",
    "    X = np.concatenate((X_numerical, X_categorical), axis=1)\n",
    "    t = original_criteo_df['treatment'].to_numpy()\n",
    "    y = original_criteo_df['visit'].to_numpy()\n",
    "    \n",
    "    return X, t, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pehe(tau_true, tau_pred):\n",
    "    return np.mean(np.square(tau_true - tau_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treatment bias selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_treatment(X, beta_sigmoid, delta):\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    X_scaled = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "    \n",
    "    proba = ((1 - 2 * delta) / (1. + np.exp(-np.dot(X_scaled, beta_sigmoid)))) + delta\n",
    "    \n",
    "    t = np.random.binomial(n=[1]*N, p=proba, size=N)\n",
    "    \n",
    "    return proba, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial_treatment(X, p):\n",
    "    \n",
    "    N = X.shape[0]\n",
    "    \n",
    "    t = np.random.binomial(n=1, p=p, size=N).reshape((N,))\n",
    "\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi-synthetic surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_surface_IHDP(X, tau=4.0, noise_scale=1e0):\n",
    "    \n",
    "    np.random.seed()\n",
    "    \n",
    "    N, d = X.shape\n",
    "    \n",
    "    beta_A = np.random.choice(a=[0,1,2,3,4], size=d, replace=True, p=[0.5, 0.2, 0.15, 0.1, 0.05])\n",
    "    \n",
    "    # CONTROL\n",
    "    # Perfect control label\n",
    "    mu_0 = np.dot(X, beta_A) \n",
    "    \n",
    "    # Adding noise to perfect control labels\n",
    "    y_0 = np.random.normal(loc=mu_0, scale=noise_scale).reshape((N, 1))\n",
    "    \n",
    "    # TREATMENT \n",
    "    # Perfect treatment label\n",
    "    mu_1 = mu_0 + tau\n",
    "    \n",
    "    # Adding noise to perfect treatment labels\n",
    "    y_1 = np.random.normal(loc=mu_1, scale=noise_scale).reshape((N, 1))\n",
    "\n",
    "    tau_f = mu_1 - mu_0      \n",
    "    \n",
    "    # Uplift with measurement noise\n",
    "    tau_true = y_1 - y_0\n",
    "\n",
    "    return y_0, y_1, tau_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_surface_IHDP(X, t, tau=4.0, noise_scale=1e0):\n",
    "    \n",
    "    np.random.seed()\n",
    "    \n",
    "    N, d = X.shape\n",
    "    \n",
    "    beta_B = np.random.choice(a=[0.0,0.1,0.2,0.3,0.4], size=d, replace=True, p=[0.6, 0.1, 0.1, 0.1, 0.1])\n",
    "    \n",
    "    W = 0.5 * np.ones_like(X)\n",
    "    \n",
    "    # CONTROL\n",
    "    # Perfect control label\n",
    "    mu_0 = np.exp(np.dot((X + W), beta_B))\n",
    "    \n",
    "    # Adding noise to perfect control labels\n",
    "    y_0 = np.random.normal(loc=mu_0, scale=noise_scale).reshape((N, 1))\n",
    "    \n",
    "    # TREATMENT \n",
    "    # Perfect treatment label\n",
    "    \n",
    "    mu_1 = np.dot(X, beta_B)\n",
    "    \n",
    "    omega = np.mean(mu_1[t == 1] - mu_0[t == 1]) - tau\n",
    "    mu_1 -= omega\n",
    "    \n",
    "    \n",
    "    # Adding noise to perfect treatment labels\n",
    "    y_1 = np.random.normal(loc=mu_1, scale=noise_scale).reshape((N, 1))\n",
    "\n",
    "    tau_f = mu_1 - mu_0      \n",
    "    \n",
    "    # Uplift with measurement noise\n",
    "    tau_true = y_1 - y_0\n",
    "\n",
    "    \n",
    "    return y_0, y_1, tau_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ours_multi_exp(X, t, nb_centers, std_scaler, ate_real, noise_scale, tol):\n",
    "    \n",
    "    np.random.seed()\n",
    "    \n",
    "    N, d = X.shape\n",
    "    stds = std_scaler * np.ones(nb_centers)\n",
    "    weights_control = np.random.uniform(low=0., high=1.0, size=nb_centers)\n",
    "    weights_treatment = np.random.uniform(low=0., high=1.0, size=nb_centers)\n",
    "    \n",
    "    idx_centers = np.random.choice(range(N), size=nb_centers)\n",
    "    centers = X[idx_centers,:] \n",
    "\n",
    "    mu_0 = np.zeros((N,))\n",
    "    \n",
    "    for idx_peak in range(nb_centers):\n",
    "        mu_0 += weights_control[idx_peak] * np.exp(-np.linalg.norm(centers[idx_peak,:]-X, axis=1)**2 / (2*stds[idx_peak]**2))\n",
    "    \n",
    "    mu_1 = np.zeros((N,))\n",
    "    \n",
    "    for idx_peak in range(nb_centers):\n",
    "        mu_1 += weights_treatment[idx_peak]* np.exp(-np.linalg.norm(centers[idx_peak,:]-X, axis=1)**2 / (2*stds[idx_peak]**2))\n",
    "        \n",
    "    weights_control_initial = np.copy(weights_control)\n",
    "    ate = np.mean(mu_1 - mu_0)\n",
    "    low = -8.0\n",
    "    high = 8.0\n",
    "    mid = (low + high) / 2.0\n",
    "    while (abs(ate - ate_real) > tol) :\n",
    "        \n",
    "        print(\"ATE ratio\")\n",
    "        print(abs(ate - ate_real)/abs(ate_real))\n",
    "        \n",
    "        weights_control = weights_control_initial + mid\n",
    "        \n",
    "        mu_0 = np.zeros((N,))\n",
    "        for idx_peak in range(nb_centers):\n",
    "            mu_0 += weights_control[idx_peak] * np.exp(-np.linalg.norm(centers[idx_peak,:]-X, axis=1)/stds[idx_peak])\n",
    "\n",
    "        ate = np.mean(mu_1 - mu_0)\n",
    "        \n",
    "        if ate > ate_real :\n",
    "            low = mid\n",
    "        else :\n",
    "            high = mid\n",
    "        \n",
    "        mid = (low + high) / 2.0\n",
    "        \n",
    "    y_0 = np.random.normal(loc=mu_0, scale=noise_scale).reshape((N, 1))\n",
    "    y_1 = np.random.normal(loc=mu_1, scale=noise_scale).reshape((N, 1))\n",
    "    \n",
    "    # Uplift without measurement noise\n",
    "    tau_f = mu_1 - mu_0      \n",
    "    \n",
    "    # Uplift with measurement noise\n",
    "    tau_true = y_1 - y_0\n",
    "    \n",
    "    return y_0, y_1, tau_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalml.inference.meta import BaseXRegressor, BaseDRRegressor, BaseTRegressor, BaseRRegressor\n",
    "from causalml.inference.tree import UpliftRandomForestClassifier, CausalTreeRegressor\n",
    "from causalml.inference.nn import CEVAE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers, callbacks\n",
    "from tensorflow.keras.callbacks import Callback, History, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, clone_model, Model\n",
    "from tensorflow.keras.layers import Input, Lambda, Layer, Dense, Activation, Dropout, Concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import glorot_uniform, Constant\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "from tensorflow.python.framework import dtypes\n",
    "tf.compat.v1.disable_eager_execution()  ### for TARNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TF Utils\n",
    "def xavier_init(size):\n",
    "    \"\"\"Xavier initialization function.\n",
    "\n",
    "    Args:\n",
    "    - size: input data dimension\n",
    "    \"\"\"\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random.normal(shape = size, stddev = tf.cast(xavier_stddev, dtypes.float32))\n",
    "\n",
    "# Mini-batch generation\n",
    "def batch_generator(x, t, y, size):\n",
    "    \"\"\" Generate mini-batch with x, t, and y.\n",
    "\n",
    "    Args:\n",
    "    - x: features\n",
    "    - t: treatments\n",
    "    - y: observed labels\n",
    "    - size: mini batch size\n",
    "\n",
    "    Returns:\n",
    "    - X_mb: mini-batch features\n",
    "    - T_mb: mini-batch treatments\n",
    "    - Y_mb: mini-batch observed labels\n",
    "    \"\"\"\n",
    "    batch_idx = np.random.randint(0, x.shape[0], size)\n",
    "\n",
    "    X_mb = x[batch_idx, :]\n",
    "    T_mb = np.reshape(t[batch_idx], [size,1]) \n",
    "    Y_mb = np.reshape(y[batch_idx], [size,1])   \n",
    "    return X_mb, T_mb, Y_mb\n",
    "\n",
    "def count_params():\n",
    "    return np.sum([np.prod(v.get_shape().as_list()) for v in tf.compat.v1.trainable_variables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TARNet (+ wasserstein f-n for CFRNet)\n",
    "\n",
    "def wasserstein(X,t,p,lam=10,its=10,sq=False,backpropT=False):\n",
    "    \"\"\" Returns the Wasserstein distance between treatment groups \"\"\"\n",
    "\n",
    "    it = tf.where(t>0)[:,0]\n",
    "    ic = tf.where(t<1)[:,0]\n",
    "    Xc = tf.gather(X,ic)\n",
    "    Xt = tf.gather(X,it)\n",
    "    nc = tf.to_float(tf.shape(Xc)[0])\n",
    "    nt = tf.to_float(tf.shape(Xt)[0])\n",
    "\n",
    "    ''' Compute distance matrix'''\n",
    "    if sq:\n",
    "        M = pdist2sq(Xt,Xc)\n",
    "    else:\n",
    "        M = safe_sqrt(pdist2sq(Xt,Xc))\n",
    "\n",
    "    ''' Estimate lambda and delta '''\n",
    "    M_mean = tf.reduce_mean(M)\n",
    "    M_drop = tf.nn.dropout(M,10/(nc*nt))\n",
    "    delta = tf.stop_gradient(tf.reduce_max(M))\n",
    "    eff_lam = tf.stop_gradient(lam/M_mean)\n",
    "\n",
    "    ''' Compute new distance matrix '''\n",
    "    Mt = M\n",
    "    row = delta*tf.ones(tf.shape(M[0:1,:]))\n",
    "    col = tf.concat(0,[delta*tf.ones(tf.shape(M[:,0:1])),tf.zeros((1,1))])\n",
    "    Mt = tf.concat(0,[M,row])\n",
    "    Mt = tf.concat(1,[Mt,col])\n",
    "\n",
    "    ''' Compute marginal vectors '''\n",
    "    a = tf.concat(0,[p*tf.ones(tf.shape(tf.where(t>0)[:,0:1]))/nt, (1-p)*tf.ones((1,1))])\n",
    "    b = tf.concat(0,[(1-p)*tf.ones(tf.shape(tf.where(t<1)[:,0:1]))/nc, p*tf.ones((1,1))])\n",
    "\n",
    "    ''' Compute kernel matrix'''\n",
    "    Mlam = eff_lam*Mt\n",
    "    K = tf.exp(-Mlam) + 1e-6 # added constant to avoid nan\n",
    "    U = K*Mt\n",
    "    ainvK = K/a\n",
    "\n",
    "    u = a\n",
    "    for i in range(0,its):\n",
    "        u = 1.0/(tf.matmul(ainvK,(b/tf.transpose(tf.matmul(tf.transpose(u),K)))))\n",
    "    v = b/(tf.transpose(tf.matmul(tf.transpose(u),K)))\n",
    "\n",
    "    T = u*(tf.transpose(v)*K)\n",
    "\n",
    "    if not backpropT:\n",
    "        T = tf.stop_gradient(T)\n",
    "\n",
    "    E = T*Mt\n",
    "    D = 2*tf.reduce_sum(E)\n",
    "\n",
    "    return D, Mlam\n",
    "\n",
    "def tarnet_model(X, y, t, tr_i, nb_layers, nb_units, nb_epochs, bs, lr, reg, weights_seed):\n",
    "    '''\n",
    "    Predict uplift scores by TARNet method.\n",
    "    '''\n",
    "    # --- initialize model ---\n",
    "    K.clear_session()\n",
    "    x_input = Input(shape=(X.shape[1],), name='x_input')\n",
    "    t_input = Input(shape=(1,), name='t_input')\n",
    "    x = x_input\n",
    "    \n",
    "    for i in range(nb_layers):\n",
    "        x = Dense(\n",
    "            nb_units,\n",
    "            kernel_regularizer=l2(reg),\n",
    "            kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "            bias_regularizer=l2(reg),\n",
    "            activation=None,\n",
    "            name='dense_'+str(i+1)\n",
    "        )(x)\n",
    "        x = Activation(('elu'))(x)\n",
    "#         x = Dropout(do)(x)\n",
    "    x_out = x\n",
    "\n",
    "\n",
    "    T_cond = lambda k: tf.gather(k, tf.cast(tf.where(t_input > 0)[:,0], tf.int32))\n",
    "    C_cond = lambda k: tf.gather(k, tf.cast(tf.where(t_input < 1)[:,0], tf.int32))\n",
    "    \n",
    "    x_T = Lambda(T_cond, name='T_input')(x_out)\n",
    "    \n",
    "    for i in range(nb_layers):\n",
    "        x_T = Dense(\n",
    "            nb_units,\n",
    "            kernel_regularizer=l2(reg),\n",
    "            kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "            bias_regularizer=l2(reg),\n",
    "            activation=None,\n",
    "            name='dense_T_'+str(i+1)\n",
    "        )(x_T)\n",
    "        x_T = Activation(('elu'))(x_T)\n",
    "#         x_T = Dropout(do)(x_T)\n",
    "    x_T = Dense(\n",
    "        1,\n",
    "        kernel_regularizer=l2(reg),\n",
    "        kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "        bias_regularizer=l2(reg),\n",
    "        activation='linear',\n",
    "        name='output_T'\n",
    "    )(x_T)\n",
    "\n",
    "    x_C = Lambda(C_cond, name='C_input')(x_out)\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x_C = Dense(\n",
    "            nb_units,\n",
    "            kernel_regularizer=l2(reg),\n",
    "            kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "            bias_regularizer=l2(reg),\n",
    "            activation=None,\n",
    "            name='dense_C_'+str(i+1)\n",
    "        )(x_C)\n",
    "        x_C = Activation(('elu'))(x_C)\n",
    "#         x_C = Dropout(do)(x_C)\n",
    "    x_C = Dense(\n",
    "        1,\n",
    "        kernel_regularizer=l2(reg),\n",
    "        kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "        bias_regularizer=l2(reg),\n",
    "        activation='linear',\n",
    "        name='output_C'\n",
    "    )(x_C)\n",
    "\n",
    "    stitch = lambda k: tf.dynamic_stitch(\n",
    "        [tf.cast(tf.where(t_input > 0)[:,0], tf.int32), tf.cast(tf.where(t_input < 1)[:,0], tf.int32)],\n",
    "        [k[0], k[1]]\n",
    "    )\n",
    "    \n",
    "    out = Lambda(stitch, name='main_output')([x_T, x_C])\n",
    "\n",
    "    model = Model(inputs=[x_input, t_input], outputs=[out])\n",
    "#     lr_schedule = optimizers.schedules.ExponentialDecay(lr, 100, 0.97)\n",
    "    lr_schedule = tf.compat.v1.train.exponential_decay(\n",
    "        global_step=0,\n",
    "        learning_rate=lr,\n",
    "        decay_steps=100,\n",
    "        decay_rate=0.97\n",
    "    )\n",
    "    optim = tf.compat.v1.train.AdamOptimizer(learning_rate=lr_schedule)\n",
    "#     optim = optimizers.Adam(lr=lr_schedule)#, decay=lr/nb_epochs)\n",
    "#     optim = optimizers.Adam(lr=lr, decay=lr/nb_epochs)\n",
    "\n",
    "    loss = 'mean_squared_error'#'binary_crossentropy'\n",
    "\n",
    "    model.compile(\n",
    "        experimental_run_tf_function=False,\n",
    "        optimizer=optim,\n",
    "        loss=loss\n",
    "    )\n",
    "\n",
    "    # --- train model ---\n",
    "    np.random.seed(0)\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=30, min_delta=1e-6, mode='min', verbose=0)\n",
    "\n",
    "    h = model.fit(\n",
    "        [X[tr_i], t[tr_i]],\n",
    "        y[tr_i],\n",
    "        epochs=nb_epochs,\n",
    "        batch_size=bs,\n",
    "        verbose=0,\n",
    "        callbacks=[es],\n",
    "        validation_split=0.2\n",
    "#         validation_data=([X[val_i], t[val_i]], y[val_i])\n",
    "    )\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist2sq(A, B):\n",
    "    #helper for PEHEnn and rbf_kernel\n",
    "    #calculates squared euclidean distance between rows of two matrices  \n",
    "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
    "    # squared norms of each row in A and B\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    # return pairwise euclidean difference matrix\n",
    "    D=tf.reduce_sum((tf.expand_dims(A, 1)-tf.expand_dims(B, 0))**2,2) \n",
    "    return D\n",
    "\n",
    "class CFRNet_Loss(Loss):\n",
    "    #initialize instance attributes\n",
    "    def __init__(self, alpha=1.,sigma=1.):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha # balances regression loss and MMD IPM\n",
    "        self.rbf_sigma=sigma #for gaussian kernel\n",
    "        self.name='cfrnet_loss'\n",
    "    def split_pred(self,concat_pred):\n",
    "        #generic helper to make sure we dont make mistakes\n",
    "        preds={}\n",
    "        preds['y0_pred'] = concat_pred[:, 0]\n",
    "        preds['y1_pred'] = concat_pred[:, 1]\n",
    "        preds['phi'] = concat_pred[:, 2:]\n",
    "        return preds\n",
    "\n",
    "    def rbf_kernel(self, A, B):\n",
    "        return tf.exp(-pdist2sq(A,B)/tf.square(self.rbf_sigma)*.5)\n",
    "\n",
    "    def calc_mmdsq(self, Phi, t):\n",
    "        Phic, Phit =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(t),tf.int32),2)\n",
    "        Kcc = self.rbf_kernel(Phic,Phic)\n",
    "        Kct = self.rbf_kernel(Phic,Phit)\n",
    "        Ktt = self.rbf_kernel(Phit,Phit)\n",
    "\n",
    "        m = tf.cast(tf.shape(Phic)[0],Phi.dtype)\n",
    "        n = tf.cast(tf.shape(Phit)[0],Phi.dtype)\n",
    "\n",
    "        mmd = 1.0/(m*(m-1.0))*(tf.reduce_sum(Kcc)-m)\n",
    "        mmd = mmd + 1.0/(n*(n-1.0))*(tf.reduce_sum(Ktt)-n)\n",
    "        mmd = mmd - 2.0/(m*n)*tf.reduce_sum(Kct)\n",
    "        return mmd * tf.ones_like(t)\n",
    "\n",
    "    def mmdsq_loss(self, concat_true,concat_pred):\n",
    "        t_true = concat_true[:, 1]\n",
    "        p=self.split_pred(concat_pred)\n",
    "        mmdsq_loss = tf.reduce_mean(self.calc_mmdsq(p['phi'],t_true))\n",
    "        return mmdsq_loss\n",
    "\n",
    "    def regression_loss(self,concat_true,concat_pred):\n",
    "        y_true = concat_true[:, 0]\n",
    "        t_true = concat_true[:, 1]\n",
    "        p = self.split_pred(concat_pred)\n",
    "        loss0 = tf.reduce_mean((1. - t_true) * tf.square(y_true - p['y0_pred']))\n",
    "        loss1 = tf.reduce_mean(t_true * tf.square(y_true - p['y1_pred']))\n",
    "        return loss0+loss1\n",
    "\n",
    "    def cfr_loss(self,concat_true,concat_pred):\n",
    "        lossR = self.regression_loss(concat_true,concat_pred)\n",
    "        lossIPM = self.mmdsq_loss(concat_true,concat_pred)\n",
    "        return lossR + self.alpha * lossIPM\n",
    "\n",
    "    #compute loss\n",
    "    def call(self, concat_true, concat_pred):        \n",
    "        return self.cfr_loss(concat_true,concat_pred)\n",
    "\n",
    "def cfrnet_model(X, y, t, tr_i, nb_layers, nb_units, nb_epochs, bs, lr, reg, weights_seed, cfr_term=0):\n",
    "\n",
    "    K.clear_session()\n",
    "    x_input = Input(shape=(X.shape[1],), name='input')\n",
    "    x = x_input\n",
    "\n",
    "    for i in range(nb_layers):\n",
    "        x = Dense(\n",
    "            nb_units,\n",
    "            kernel_regularizer=l2(reg),\n",
    "            kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "            bias_regularizer=l2(reg),\n",
    "            activation=None,\n",
    "            name='dense_'+str(i+1)\n",
    "        )(x)\n",
    "\n",
    "    phi = x\n",
    "\n",
    "    x_T = phi\n",
    "    for i in range(nb_layers):\n",
    "        x_T = Dense(\n",
    "            nb_units,\n",
    "            kernel_regularizer=l2(reg),\n",
    "            kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "            bias_regularizer=l2(reg),\n",
    "            activation=None,\n",
    "            name='dense_T_'+str(i+1)\n",
    "        )(x_T)\n",
    "        x_T = Activation(('elu'))(x_T)\n",
    "\n",
    "    x_C = phi\n",
    "    for i in range(nb_layers):\n",
    "        x_C = Dense(\n",
    "            nb_units,\n",
    "            kernel_regularizer=l2(reg),\n",
    "            kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "            bias_regularizer=l2(reg),\n",
    "            activation=None,\n",
    "            name='dense_C_'+str(i+1)\n",
    "        )(x_C)\n",
    "        x_C = Activation(('elu'))(x_C)\n",
    "\n",
    "    y_T_pred = Dense(\n",
    "        1,\n",
    "        kernel_regularizer=l2(reg),\n",
    "        kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "        bias_regularizer=l2(reg),\n",
    "        activation='linear',\n",
    "        name='output_T'\n",
    "    )(x_T)\n",
    "\n",
    "    y_C_pred = Dense(\n",
    "        1,\n",
    "        kernel_regularizer=l2(reg),\n",
    "        kernel_initializer=glorot_uniform(seed=weights_seed),\n",
    "        bias_regularizer=l2(reg),\n",
    "        activation='linear',\n",
    "        name='output_C'\n",
    "    )(x_C)\n",
    "    concat_pred = Concatenate(1)([y_C_pred, y_T_pred, phi])\n",
    "    model = Model(inputs=x_input, outputs=concat_pred)\n",
    "\n",
    "    tf.random.set_seed(weights_seed)\n",
    "    np.random.seed(weights_seed)\n",
    "\n",
    "    yt = np.stack([y[tr_i], t[tr_i]], 1)\n",
    "\n",
    "    adam_callbacks = [\n",
    "            TerminateOnNaN(),\n",
    "            EarlyStopping(monitor='val_loss', patience=30, min_delta=1e-6, mode='min', verbose=0),\n",
    "            ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=0, mode='auto', min_delta=1e-8, cooldown=0, min_lr=0),\n",
    "        ]\n",
    "\n",
    "    cfrnet_loss = CFRNet_Loss(alpha=cfr_term)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(lr=lr),\n",
    "        loss=cfrnet_loss,\n",
    "    )\n",
    "\n",
    "    h = model.fit(\n",
    "        x=X[tr_i],\n",
    "        y=yt,  \n",
    "        callbacks=adam_callbacks,\n",
    "        validation_split=0.2,\n",
    "        epochs=nb_epochs,\n",
    "        batch_size=bs,\n",
    "        verbose=0\n",
    "    )\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation_fit_test_predict_TARnet(X, y, t, tau_f, trval_i, test_i, params_dico, nb_trials, nb_cv):\n",
    "    \n",
    "    ### Handling parameters\n",
    "    params_mesh = np.array(np.meshgrid(*params_dico.values()))\n",
    "    params_mesh = params_mesh.T.reshape(-1, params_mesh.shape[0])\n",
    "    n_params = len(params_dico.keys())\n",
    "    all_comb = params_mesh.shape[0]\n",
    "    nb_sampled_comb = np.min([all_comb, nb_trials])\n",
    "    rand_combs = np.random.permutation(all_comb)[:nb_sampled_comb]\n",
    "    \n",
    "    ### Generating crossvalidation splits\n",
    "    kf = KFold(n_splits=nb_cv)\n",
    "    \n",
    "    ### Fitting\n",
    "    params_cv = []\n",
    "    for comb in rand_combs:\n",
    "        nb_l, nb_u, bs, lr, reg = params_mesh[comb, :]\n",
    "        pehe_at_comb_val = []\n",
    "        for tr_i, val_i in kf.split(X[trval_i]):\n",
    "            h = tarnet_model(X[trval_i], y[trval_i], t[trval_i], tr_i, nb_layers=int(nb_l), nb_units=int(nb_u), nb_epochs=20, bs=int(bs), lr=lr, reg=reg, weights_seed=0)\n",
    "            y_pred_t_val = np.squeeze(h.model.predict([X[trval_i][val_i], np.ones((len(X[trval_i][val_i]), 1))]))\n",
    "            y_pred_c_val = np.squeeze(h.model.predict([X[trval_i][val_i], np.zeros((len(X[trval_i][val_i]), 1))]))\n",
    "            u_val = y_pred_t_val-y_pred_c_val\n",
    "\n",
    "            pehe_val = pehe(tau_f[trval_i][val_i], u_val)\n",
    "            #print(pehe_val)\n",
    "            pehe_at_comb_val.append(pehe_val)\n",
    "        params_cv.append([comb, np.mean(pehe_at_comb_val)])\n",
    "        \n",
    "    nb_l, nb_u, bs, lr, reg = params_mesh[min(params_cv, key=lambda x: x[-1])[0], :]\n",
    "    h = tarnet_model(X, y, t, trval_i, nb_layers=int(nb_l), nb_units=int(nb_u), nb_epochs=100, bs=int(bs), lr=lr, reg=reg, weights_seed=0)\n",
    "\n",
    "    y_pred_t_test = np.squeeze(h.model.predict([X[test_i], np.ones((len(X[test_i]), 1))]))\n",
    "    y_pred_c_test = np.squeeze(h.model.predict([X[test_i], np.zeros((len(X[test_i]), 1))]))\n",
    "    u_test = y_pred_t_test-y_pred_c_test\n",
    "    pehe_test = pehe(tau_f[test_i], u_test)\n",
    "    return u_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation_fit_test_predict_CFRnet(X, y, t, tau_f, trval_i, test_i, params_dico, nb_trials, nb_cv):\n",
    "    \n",
    "    ### Handling parameters\n",
    "    params_mesh = np.array(np.meshgrid(*params_dico.values()))\n",
    "    params_mesh = params_mesh.T.reshape(-1, params_mesh.shape[0])\n",
    "    n_params = len(params_dico.keys())\n",
    "    all_comb = params_mesh.shape[0]\n",
    "    nb_sampled_comb = np.min([all_comb, nb_trials])\n",
    "    rand_combs = np.random.permutation(all_comb)[:nb_sampled_comb]\n",
    "    \n",
    "    ### Generating crossvalidation splits\n",
    "    kf = KFold(n_splits=nb_cv)\n",
    "    \n",
    "    ### Fitting\n",
    "    params_cv = []\n",
    "    for comb in rand_combs:\n",
    "        nb_l, nb_u, bs, lr, reg, cfr_term = params_mesh[comb, :]\n",
    "        pehe_at_comb_val = []\n",
    "        for tr_i, val_i in kf.split(X[trval_i], t[trval_i]):\n",
    "            \n",
    "            h = cfrnet_model(X[trval_i], y[trval_i].reshape((len(trval_i),)), t[trval_i].reshape((len(trval_i),)), tr_i, \n",
    "                             nb_layers=int(nb_l), nb_units=int(nb_u), nb_epochs=20, \n",
    "                             bs=int(bs), lr=lr, reg=reg, weights_seed=0, cfr_term=float(cfr_term))\n",
    "            \n",
    "            y_pred_t_val = h.model.predict(X[trval_i][val_i])[:,1]\n",
    "            y_pred_c_val = h.model.predict(X[trval_i][val_i])[:,0]\n",
    "            \n",
    "            \n",
    "            u_val = y_pred_t_val - y_pred_c_val\n",
    "\n",
    "            pehe_val = pehe(tau_f[trval_i][val_i], u_val)\n",
    "\n",
    "            pehe_at_comb_val.append(pehe_val)\n",
    "        params_cv.append([comb, np.mean(pehe_at_comb_val)])\n",
    "        \n",
    "    nb_l, nb_u, bs, lr, reg, cfr_term = params_mesh[min(params_cv, key=lambda x: x[-1])[0], :]\n",
    "    \n",
    "    h = cfrnet_model(X, y.reshape((y.shape[0],)), t.reshape((y.shape[0],)), trval_i, \n",
    "                     nb_layers=int(nb_l), nb_units=int(nb_u), nb_epochs=100, \n",
    "                     bs=int(bs), lr=lr, reg=reg, weights_seed=0, cfr_term=float(cfr_term))\n",
    "\n",
    "    y_pred_t_test = h.model.predict(X[test_i])[:,1]\n",
    "    y_pred_c_test = h.model.predict(X[test_i])[:,0]\n",
    "    u_test = y_pred_t_test-y_pred_c_test\n",
    "    pehe_test = pehe(tau_f[test_i], u_test)\n",
    "    return u_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation_fit_test_predict_CausalTree(X, y, t, tau_f, trval_i, test_i, params_dico, nb_trials, nb_cv):\n",
    "    \n",
    "    ### Handling parameters\n",
    "    params_mesh = np.array(np.meshgrid(*params_dico.values()))\n",
    "    params_mesh = params_mesh.T.reshape(-1, params_mesh.shape[0])\n",
    "    n_params = len(params_dico.keys())\n",
    "    all_comb = params_mesh.shape[0]\n",
    "    nb_sampled_comb = np.min([all_comb, nb_trials])\n",
    "    rand_combs = np.random.permutation(all_comb)[:nb_sampled_comb]\n",
    "    \n",
    "    ### Generating crossvalidation splits\n",
    "    kf = KFold(n_splits=nb_cv)\n",
    "    \n",
    "    ### Fitting\n",
    "    params_cv = []\n",
    "    for comb in rand_combs:\n",
    "        max_depth, min_sample_leaf = params_mesh[comb, :]\n",
    "        pehe_at_comb_val = []\n",
    "        for tr_i, val_i in kf.split(X[trval_i]):\n",
    "            causaltree_model = CausalTreeRegressor(max_depth=int(max_depth), min_samples_leaf=int(min_sample_leaf))\n",
    "            \n",
    "            causaltree_model.fit(X=X[trval_i][tr_i], \n",
    "                                 treatment=t[trval_i][tr_i], \n",
    "                                 y=y[trval_i][tr_i])\n",
    "\n",
    "            u_val = causaltree_model.predict(X[trval_i][val_i])\n",
    "\n",
    "            pehe_val = pehe(tau_f[trval_i][val_i], u_val)\n",
    "\n",
    "            pehe_at_comb_val.append(pehe_val)\n",
    "        params_cv.append([comb, np.mean(pehe_at_comb_val)])\n",
    "        \n",
    "    max_depth, min_sample_leaf = params_mesh[min(params_cv, key=lambda x: x[-1])[0], :]\n",
    "    causaltree_model = CausalTreeRegressor(max_depth=int(max_depth), min_samples_leaf=int(min_sample_leaf))\n",
    "    causaltree_model.fit(X=X[trval_i], \n",
    "                         treatment=t[trval_i], \n",
    "                         y=y[trval_i])\n",
    "\n",
    "    u_test = causaltree_model.predict(X[test_i])\n",
    "    \n",
    "    pehe_test = pehe(tau_f[test_i], u_test)\n",
    "    \n",
    "    return u_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidation_fit_test_predict_CEVAE(X, y, t, tau_f, trval_i, test_i, params_dico, nb_trials, nb_cv):\n",
    "    \n",
    "    ### Handling parameters\n",
    "    params_mesh = np.array(np.meshgrid(*params_dico.values()))\n",
    "    params_mesh = params_mesh.T.reshape(-1, params_mesh.shape[0])\n",
    "    n_params = len(params_dico.keys())\n",
    "    all_comb = params_mesh.shape[0]\n",
    "    nb_sampled_comb = np.min([all_comb, nb_trials])\n",
    "    rand_combs = np.random.permutation(all_comb)[:nb_sampled_comb]\n",
    "    \n",
    "    ### Generating crossvalidation splits\n",
    "    kf = KFold(n_splits=nb_cv)\n",
    "    \n",
    "    ### Fitting\n",
    "    params_cv = []\n",
    "    for comb in rand_combs:\n",
    "        lat_dim, hid_dim, nb_layer, bs, lr = params_mesh[comb, :]\n",
    "        pehe_at_comb_val = []\n",
    "        for tr_i, val_i in kf.split(X[trval_i]):\n",
    "            CEVAE_model = CEVAE(latent_dim=int(lat_dim), hidden_dim=int(hid_dim), num_epochs=10, num_layers=int(nb_layer), batch_size=int(bs), learning_rate=float(lr))\n",
    "            \n",
    "            CEVAE_model.fit(X=X[trval_i][tr_i], \n",
    "                                 treatment=t[trval_i][tr_i], \n",
    "                                 y=y[trval_i][tr_i].reshape((len(tr_i),)))\n",
    "\n",
    "            u_val = CEVAE_model.predict(X[trval_i][val_i])\n",
    "\n",
    "            pehe_val = pehe(tau_f[trval_i][val_i], u_val)\n",
    "\n",
    "            pehe_at_comb_val.append(pehe_val)\n",
    "        params_cv.append([comb, np.mean(pehe_at_comb_val)])\n",
    "        \n",
    "    lat_dim, hid_dim, nb_layer, bs, lr = params_mesh[min(params_cv, key=lambda x: x[-1])[0], :]\n",
    "    CEVAE_model = CEVAE(latent_dim=int(lat_dim), hidden_dim=int(hid_dim), num_epochs=20, num_layers=int(nb_layer), batch_size=int(bs), learning_rate=float(lr))\n",
    "    CEVAE_model.fit(X=X[trval_i], \n",
    "                     treatment=t[trval_i], \n",
    "                     y=y[trval_i].reshape((len(trval_i),)))\n",
    "\n",
    "    u_test = CEVAE_model.predict(X[test_i])\n",
    "    \n",
    "    pehe_test = pehe(tau_f[test_i], u_test)\n",
    "    \n",
    "    return u_test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X, t, y = load_hashed_criteo(nb_categorical_features_kept=6)\n",
    "N, d = X.shape\n",
    "N_data = 10000\n",
    "train_perc = 0.5\n",
    "nb_trials = 2\n",
    "nb_cv = 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "N, d = X.shape\n",
    "beta_sigmoid = np.zeros(d)\n",
    "beta_sigmoid[2] = 1.0\n",
    "proba, t = sigmoid_treatment(X, beta_sigmoid, delta)\n",
    "\n",
    "y_0, y_1, tau_f = linear_surface_IHDP(X, tau=4.0, noise_scale=1.0)\n",
    "\n",
    "t = t.reshape((N,1))\n",
    "\n",
    "y = y_0*(1-t) + y_1*t\n",
    "\n",
    "t = t.reshape((N,))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "params = {}\n",
    "params['nb_l'] = [2]\n",
    "params['nb_u'] = [64]\n",
    "params['bs'] = [128]\n",
    "params['lr'] = [1e-3]\n",
    "params['reg'] = [1e-6]\n",
    "params['cfr_term'] = [0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_i = np.random.choice(range(N), size=N_data)\n",
    "train_i, test_i = data_i[range(int(train_perc*N_data))], data_i[range(int(train_perc*N_data), N_data)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "u_val = crossvalidation_fit_test_predict_CFRnet(X, y, t, tau_f, \n",
    "                                                train_i, test_i, \n",
    "                                                params, nb_trials, nb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose treatment bias"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X, t, y = load_hashed_criteo(nb_categorical_features_kept=6)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "beta_sigmoid = np.zeros(X.shape[1])\n",
    "beta_sigmoid[2] = 1.0\n",
    "delta = 0.01"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prop_score, t_biased = sigmoid_treatment(X, beta_sigmoid, delta)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.clf()\n",
    "plt.hist(X[t_biased==0, 2], bins=1000, color='b', alpha=0.4, label='Control')\n",
    "plt.hist(X[t_biased==1, 2], bins=1000, color='r', alpha=0.4, label='Treatment')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Number of individuals', fontsize=16)\n",
    "plt.xlabel('$X_0$', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "plt.clf()\n",
    "plt.hist(X[:, 2], bins=1000, color='g', alpha=0.40, label='Total')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Number of individuals', fontsize=16)\n",
    "plt.xlabel('$X_0$', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_synthetic_uplift(n_experiments, model_list, nb_categorical_features_kept, N_data, \n",
    "                                treatment_setup, delta, p,\n",
    "                                synthetic_setup, nb_centers, std_scaler,\n",
    "                                train_perc, nb_trials, nb_cv):\n",
    "    \n",
    "    pickle.dump(model_list, open('RF_experiment_' + str(n_experiments) + '_' + str(N_data) + '_' + synthetic_setup +'_models.pickle', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    ### Initialization\n",
    "    n_models = len(model_list)\n",
    "    PEHE_val = np.zeros((n_experiments, n_models))  \n",
    "    \n",
    "    ### Load real data from Criteo Dataset\n",
    "    X_init, t_real, y_real = load_hashed_criteo(nb_categorical_features_kept)\n",
    "    N, d = X_init.shape\n",
    "    \n",
    "    print('d = ' + str(d))\n",
    "    \n",
    "    for idx_exp in tqdm(range(n_experiments)):\n",
    "        \n",
    "        ### CHOOSE TREATMENT BIAS\n",
    "        if treatment_setup == 'sigmoid':\n",
    "            beta_sigmoid = np.zeros(d)\n",
    "            beta_sigmoid[2] = 1.0\n",
    "            proba, t = sigmoid_treatment(X_init, beta_sigmoid, delta)\n",
    "            \n",
    "        elif treatment_setup == 'binomial':\n",
    "            t = binomial_treatment(X_init, p)\n",
    "        \n",
    "        ### CHOOSE SYNTHETIC SET-UP\n",
    "        ate_real = 4.0#np.mean(y_real[t_real==1]) - np.mean(y_real[t_real==0])\n",
    "        noise_scale = ate_real/4.0\n",
    "        tol = ate_real/100.\n",
    "        \n",
    "        if synthetic_setup == 'multi_exp':\n",
    "            \n",
    "            y_0, y_1, tau_f = ours_multi_exp(X_init, t, nb_centers, std_scaler, ate_real, noise_scale, tol)\n",
    "\n",
    "        elif synthetic_setup == 'linear':\n",
    "\n",
    "            y_0, y_1, tau_f = linear_surface_IHDP(X_init, ate_real, noise_scale)\n",
    "\n",
    "        elif synthetic_setup == 'exponential':\n",
    "\n",
    "            y_0, y_1, tau_f = exp_surface_IHDP(X_init, t, ate_real, noise_scale)\n",
    "        \n",
    "        t = t.reshape((N,1))\n",
    "        \n",
    "        y = y_0*(1-t) + y_1*t\n",
    "    \n",
    "        t = t.reshape((N,))\n",
    "        \n",
    "        # Random Subsampling\n",
    "        data_i = np.random.choice(range(N), size=N_data)\n",
    "        train_i, test_i = data_i[range(int(train_perc*N_data))], data_i[range(int(train_perc*N_data), N_data)]\n",
    "        \n",
    "        X_train = X_init[train_i,:].reshape((len(train_i), d))\n",
    "        y_train = y[train_i].reshape((len(train_i),))\n",
    "        t_train = t[train_i].reshape((len(train_i),))\n",
    "        \n",
    "        X_test = X_init[test_i,:].reshape((len(test_i), d))\n",
    "        y_test = y[test_i].reshape((len(test_i), ))\n",
    "        t_test = t[test_i].reshape((len(test_i), ))\n",
    "    \n",
    "        ### POOL OF MODELS\n",
    "        for idx_model in range(n_models): \n",
    "            \n",
    "            dict_current_model = model_list[idx_model]\n",
    "            print(dict_current_model['name'])\n",
    "            \n",
    "            if dict_current_model['type'] == 'BaseEstimator':\n",
    "                \n",
    "                model_current = dict_current_model['model']\n",
    "                \n",
    "                model_current.fit(X=X_train, \n",
    "                                  treatment=t_train, \n",
    "                                  y=y_train)\n",
    "                \n",
    "                uplift_granular = model_current.predict(X_test)\n",
    "                \n",
    "            elif dict_current_model['type'] == 'SM':\n",
    "\n",
    "                # Instantiate\n",
    "                model_current = clone(dict_current_model['model'])\n",
    "                \n",
    "                # Training\n",
    "                X_train_SM = np.concatenate((X_train, t_train), axis=1)\n",
    "                model_current.fit(X_train_SM, y_train)\n",
    "                \n",
    "                # Prediction\n",
    "                X_test_SM_1 = np.concatenate((X_test, np.ones(t_train.shape)), axis=1)\n",
    "                X_test_SM_0 = np.concatenate((X_test, np.zeros(t_train.shape)), axis=1)\n",
    "                uplift_granular = model_current.predict(X_test_SM_1) - model_current.predict(X_test_SM_0)\n",
    "        \n",
    "                \n",
    "            elif dict_current_model['type'] == 'TM':\n",
    "\n",
    "                # Instantiate\n",
    "                model_current_0 = clone(dict_current_model['model'])\n",
    "                model_current_1 = clone(dict_current_model['model'])\n",
    "                \n",
    "                # Training\n",
    "                y0_train = y_train[t_train==0]\n",
    "                X0_train = X_train[t_train==0]\n",
    "                model_current_0.fit(X0_train, y0_train)\n",
    "                \n",
    "                y1_train = y_train[t_train==1]\n",
    "                X1_train = X_train[t_train==1]\n",
    "                model_current_1.fit(X1_train, y1_train)\n",
    "                \n",
    "                # Prediction\n",
    "                uplift_granular = model_current_1.predict(X_test) - model_current_0.predict(X_test)\n",
    "                \n",
    "            elif dict_current_model['type'] == 'CausalTree':\n",
    "                \n",
    "                uplift_granular = crossvalidation_fit_test_predict_CausalTree(X_init, y, t, tau_f, train_i, test_i, dict_current_model['params'], nb_trials, nb_cv)\n",
    "            \n",
    "            elif dict_current_model['type'] == 'TARnet':\n",
    "                \n",
    "                uplift_granular = crossvalidation_fit_test_predict_TARnet(X_init, y, t, tau_f, train_i, test_i, dict_current_model['params'], nb_trials, nb_cv)\n",
    "            \n",
    "            elif dict_current_model['type'] == 'CFRnet':\n",
    "                \n",
    "                uplift_granular = crossvalidation_fit_test_predict_CFRnet(X_init, y, t, tau_f, train_i, test_i, dict_current_model['params'], nb_trials, nb_cv)\n",
    "            \n",
    "            elif dict_current_model['type'] == 'CEVAE':\n",
    "                \n",
    "                uplift_granular = crossvalidation_fit_test_predict_CEVAE(X_init, y, t, tau_f, train_i, test_i, dict_current_model['params'], nb_trials, nb_cv)\n",
    "            \n",
    "            ### METRIC EVALUATION\n",
    "            PEHE_val[idx_exp,idx_model] = pehe(tau_f[test_i].reshape((len(test_i),)), uplift_granular.reshape((len(test_i),)))\n",
    "        np.save('RF_experiment_' + str(n_experiments) + '_' + str(N_data) + '_' + synthetic_setup +'_PEHE.npy', PEHE_val)\n",
    "    \n",
    "    del X_init, t_real, y_real\n",
    "    \n",
    "    return PEHE_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perc = 0.5\n",
    "\n",
    "n_experiments = 10\n",
    "nb_categorical_features_kept = 5\n",
    "N_data = 1000000\n",
    "\n",
    "### Choose treatment setup\n",
    "treatment_setup = 'sigmoid'\n",
    "p = 0.5\n",
    "delta = 0.01\n",
    "\n",
    "### Choose synthetic setup\n",
    "synthetic_setup = 'exponential'\n",
    "nb_centers = 5\n",
    "std_scaler = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating_model_list\n",
    "model_list = []\n",
    "nb_cpu = 40\n",
    "nb_trials = 5\n",
    "nb_cv = 5\n",
    "\n",
    "### Hyperparameter space definition\n",
    "cvt_params_tree = {'max_depth':[1,2,4,8,16]}\n",
    "cvt_params_RF = {'max_depth':[1,2,4,8], 'n_estimators':[10,20]}\n",
    "cvt_params_CausalTree = {'max_depth':[1,2,4,8,16], 'min_samples_leaf':[10, 50, 100, 200]}\n",
    "cvt_params_ridge = {'alpha':[0.1, 1.0, 10.0, 100.0, 1000.0]}\n",
    "cvt_params_logistic = {'C':[1.0,10.0]}\n",
    "\n",
    "### Submodel \n",
    "Tree_learner_TG = RandomizedSearchCV(\n",
    "        DecisionTreeRegressor(),\n",
    "        cvt_params_tree,\n",
    "        n_iter=nb_trials,\n",
    "        n_jobs=nb_cpu,\n",
    "        cv=nb_cv,\n",
    "    )\n",
    "\n",
    "\n",
    "RF_learner_TG = RandomizedSearchCV(\n",
    "        RandomForestRegressor(),\n",
    "        cvt_params_RF,\n",
    "        n_iter=nb_trials,\n",
    "        n_jobs=nb_cpu,\n",
    "        cv=nb_cv,\n",
    "    )\n",
    "\n",
    "Logistic_learner_TG = RandomizedSearchCV(\n",
    "        LogisticRegression(),\n",
    "        cvt_params_logistic,\n",
    "        n_iter=nb_trials,\n",
    "        n_jobs=nb_cpu,\n",
    "        cv=nb_cv,\n",
    "    )\n",
    "\n",
    "Ridge_learner_TG = RandomizedSearchCV(\n",
    "                        Ridge(),\n",
    "                        cvt_params_ridge,\n",
    "                        n_iter=nb_trials,\n",
    "                        n_jobs=nb_cpu,\n",
    "                        cv=nb_cv,\n",
    "                    )\n",
    "\n",
    "### TM \n",
    "dico_model = {}\n",
    "dico_model['name'] = 'TM-Regressor'\n",
    "dico_model['type'] = 'TM'\n",
    "dico_model['model'] = Ridge_learner_TG\n",
    "#model_list.append(dico_model)\n",
    "\n",
    "### T Learner\n",
    "dico_model = {}\n",
    "dico_model['name'] = 'T-Regressor'\n",
    "dico_model['type'] = 'BaseEstimator'\n",
    "dico_model['model'] = BaseTRegressor(learner=RF_learner_TG)\n",
    "model_list.append(dico_model)\n",
    "\n",
    "### X Learner\n",
    "dico_model = {}\n",
    "dico_model['name'] = 'X-Regressor'\n",
    "dico_model['type'] = 'BaseEstimator'\n",
    "dico_model['model'] = BaseXRegressor(learner=RF_learner_TG)\n",
    "model_list.append(dico_model)\n",
    "\n",
    "### R Learner\n",
    "dico_model = {}\n",
    "dico_model['name'] = 'R-Regressor'\n",
    "dico_model['type'] = 'BaseEstimator'\n",
    "dico_model['model'] = BaseRRegressor(learner=RF_learner_TG)\n",
    "model_list.append(dico_model)\n",
    "\n",
    "### DR Learner\n",
    "dico_model = {}\n",
    "dico_model['name'] = 'DR-Regressor'\n",
    "dico_model['type'] = 'BaseEstimator'\n",
    "dico_model['model'] = BaseDRRegressor(learner=RF_learner_TG)\n",
    "model_list.append(dico_model)\n",
    "\n",
    "### Causal Tree regressor\n",
    "dico_model = {}\n",
    "dico_model['name'] = 'CausalTree-Regressor'\n",
    "dico_model['type'] = 'CausalTree'\n",
    "dico_model['params'] = cvt_params_CausalTree\n",
    "dico_model['model'] = CausalTreeRegressor()\n",
    "#model_list.append(dico_model)\n",
    "\n",
    "### CEVAE\n",
    "dico_model = {}\n",
    "dico_model['name'] = 'CEVAE'\n",
    "dico_model['type'] = 'CEVAE'\n",
    "params = {}\n",
    "params['lat_dim'] = [10, 20, 30]\n",
    "params['hid_dim'] = [32, 64]\n",
    "params['nb_layer'] = [3]\n",
    "params['bs'] = [128, 512]\n",
    "params['lr'] = [1e-5, 1e-3, 1e-1]\n",
    "dico_model['params'] = params\n",
    "#model_list.append(dico_model)\n",
    "\n",
    "### TARnet\n",
    "dico_model = {}\n",
    "dico_model['name'] = 'TARnet'\n",
    "dico_model['type'] = 'TARnet'\n",
    "params = {}\n",
    "params['nb_l'] = [2, 3]\n",
    "params['nb_u'] = [32, 64]\n",
    "params['bs'] = [128]\n",
    "params['lr'] = [1e-3, 1e-5]\n",
    "params['reg'] = [1e-4, 1e-6]\n",
    "dico_model['params'] = params\n",
    "model_list.append(dico_model)\n",
    "\n",
    "### CFRnet\n",
    "dico_model = {}\n",
    "dico_model['name'] = 'CFRnet'\n",
    "dico_model['type'] = 'CFRnet'\n",
    "params = {}\n",
    "params['nb_l'] = [2, 3]\n",
    "params['nb_u'] = [32, 64]\n",
    "params['bs'] = [128]\n",
    "params['lr'] = [1e-3, 1e-5]\n",
    "params['reg'] = [1e-4, 1e-6]\n",
    "params['cfr_term'] = [1e-2, 1e-4]\n",
    "dico_model['params'] = params\n",
    "model_list.append(dico_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Criteo dataset...\n",
      "Criteo dataset DONE !\n",
      "d = 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Regressor\n",
      "X-Regressor\n",
      "R-Regressor\n",
      "DR-Regressor\n",
      "TARnet\n",
      "CFRnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [2:56:14<26:26:10, 10574.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-Regressor\n",
      "X-Regressor\n",
      "R-Regressor\n",
      "DR-Regressor\n",
      "TARnet\n",
      "CFRnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [5:28:45<49:18:53, 19725.95s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-931da3f96fe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                         \u001b[0mtreatment_setup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                         \u001b[0msynthetic_setup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                         train_perc, nb_trials, nb_cv)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-225c2a8bb64a>\u001b[0m in \u001b[0;36mevaluation_synthetic_uplift\u001b[0;34m(n_experiments, model_list, nb_categorical_features_kept, N_data, treatment_setup, delta, p, synthetic_setup, nb_centers, std_scaler, train_perc, nb_trials, nb_cv)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdict_current_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CFRnet'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0muplift_granular\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossvalidation_fit_test_predict_CFRnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_current_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mdict_current_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'CEVAE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-2c37ea4887b7>\u001b[0m in \u001b[0;36mcrossvalidation_fit_test_predict_CFRnet\u001b[0;34m(X, y, t, tau_f, trval_i, test_i, params_dico, nb_trials, nb_cv)\u001b[0m\n\u001b[1;32m     21\u001b[0m             h = cfrnet_model(X[trval_i], y[trval_i].reshape((len(trval_i),)), t[trval_i].reshape((len(trval_i),)), tr_i, \n\u001b[1;32m     22\u001b[0m                              \u001b[0mnb_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_u\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                              bs=int(bs), lr=lr, reg=reg, weights_seed=0, cfr_term=float(cfr_term))\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0my_pred_t_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrval_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-8f9563d6e540>\u001b[0m in \u001b[0;36mcfrnet_model\u001b[0;34m(X, y, t, tr_i, nb_layers, nb_units, nb_epochs, bs, lr, reg, weights_seed, cfr_term)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     )\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/jupyter/.python-kernel/python-kernel-43553/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/share/jupyter/.python-kernel/python-kernel-43553/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/share/jupyter/.python-kernel/python-kernel-43553/lib64/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/jupyter/.python-kernel/python-kernel-43553/lib64/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3824\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3825\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3827\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/.local/share/jupyter/.python-kernel/python-kernel-43553/lib64/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PEHE_val = evaluation_synthetic_uplift(n_experiments, model_list, nb_categorical_features_kept, N_data, \n",
    "                                        treatment_setup, delta, p,\n",
    "                                        synthetic_setup, nb_centers, std_scaler,\n",
    "                                        train_perc, nb_trials, nb_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "PEHE_val = np.load('RF_experiment_10_100000_exponential_PEHE.npy')\n",
    "PEHE_val = np.sqrt(PEHE_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = pickle.load(open('RF_experiment_10_100000_exponential_models.pickle', \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pehe = np.mean(PEHE_val, axis=0)\n",
    "std_pehe = np.std(PEHE_val, axis=0)\n",
    "sorted_models = [x for _, x in sorted(zip(mean_pehe, model_list))]\n",
    "sorted_std = [x for _, x in sorted(zip(mean_pehe, std_pehe))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFRnet - PEHE : 0.152471492 +/- 0.03212274250917466\n",
      "TARnet - PEHE : 0.195430501 +/- 0.04443802520165637\n",
      "T-Regressor - PEHE : 0.332508682 +/- 0.055622188413029225\n",
      "R-Regressor - PEHE : 0.356203168 +/- 0.04494315771626078\n",
      "DR-Regressor - PEHE : 0.379227336 +/- 0.08014386839025717\n",
      "X-Regressor - PEHE : 0.413417074 +/- 0.1367378228319574\n"
     ]
    }
   ],
   "source": [
    "sorted_mean = sorted(mean_pehe)\n",
    "for i in range(len(model_list)):\n",
    "    print(sorted_models[i]['name'] + \" - PEHE : \" + str(round(sorted_mean[i],9)) + \" +/- \" + str(round(sorted_std[i],30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.8901357 ,  1.88175449,  9.32175011,  2.67648624,  0.71514422,\n",
       "         0.24898922],\n",
       "       [ 1.8901357 ,  1.88175449, 13.43396333,  2.72371105,  0.71514422,\n",
       "         0.24898922],\n",
       "       [ 1.8901357 ,  1.88175449, 14.50687734,  2.44495526,  0.71514422,\n",
       "         0.24898922],\n",
       "       [ 1.8901357 ,  1.88789622, 13.84568594,  2.02623571,  0.71514422,\n",
       "         0.24898922],\n",
       "       [ 1.8901357 ,  1.88175449, 10.33069794,  2.87120245,  0.58535502,\n",
       "         0.15372177],\n",
       "       [ 1.8901357 ,  1.88175449, 12.68848224,  2.44495526,  0.71514422,\n",
       "         0.24898922],\n",
       "       [ 1.8901357 ,  1.88789622, 10.98040328,  2.00835087,  0.71514422,\n",
       "         0.24898922],\n",
       "       [ 1.8901357 ,  1.88175449, 14.90739274,  3.05816234,  0.51825641,\n",
       "         0.24898922],\n",
       "       [ 1.8901357 ,  1.88175449, 11.96996367,  2.70326559,  0.71514422,\n",
       "         0.24898922],\n",
       "       [ 1.8901357 ,  1.88175449,  4.69462131,  2.14536029,  0.71514422,\n",
       "         0.24898922]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PEHE_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Criteo (MOAB #43553)",
   "language": "python",
   "name": "python-kernel-43553"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
